[{"chunk_id": "rag_test_0000_81a5f666", "doc_id": "rag_test", "content": "RAG combines retrieval with generation for grounded AI responses. It uses vector embeddings for semantic search.", "metadata": {"char_count": 112, "word_count": 16}}, {"chunk_id": "2af33fabfb81da93_0000_0918f5d0", "doc_id": "2af33fabfb81da93", "content": "Memorize: AI Powered Knowledge Assistant \n \nA project report submitted in partial fulfilment of the requirements \nfor the award of the degree in \nBACHELOR OF TECHNOLOGY \nIN \nCOMPUTER SCIENCE AND ENGINEERING \n \nSubmitted by \nPANDRINKI VENKATESH                 321506402255 \nNALANATI PHANISRI SAI AKSHITH                          321506402229 \nPRATAPARAO SUMANTH                                               321506402280 \nMAHANTHI MURALI KRISHNA                                    321506402222 \nUnder the esteemed guidance of \n \nProf. K. Venkata Rao \nProfessor \n \nDEPARTMENT OF \nCOMPUTER SCIENCE AND SYSTEMS ENGINEERING \n \nANDHRA UNIVERSITY COLLEGE OF ENGINEERING (A) \nANDHRA UNIVERSITY \nVISAKHAPATNAM - 530003 \n2025", "metadata": {"char_count": 716, "word_count": 76, "page": 1}}, {"chunk_id": "2af33fabfb81da93_0001_303a607d", "doc_id": "2af33fabfb81da93", "content": "DEPARTMENT OF COMPUTER SCIENCE AND SYSTEMS ENGINEERING \nANDHRA UNIVERSITY COLLEGE OF ENGINEERING (A) \nANDHRA UNIVERSITY \nVISAKHAPATNAM-530003 \n \nCERTIFICATE \nThis is to certify that the project report entitled \u201cMemorize: A I Powered Knowledge \nAssistant\u201d is a bonafide project work done by PANDRINKI VENKATESH \n(321506402255), NALANATI PHANISRI SAI AKSHITH  (321506402229), \nPRATAPRAO SUMANTH (321506402280), MAHANTHI MURALI KRISHNA  \n(321506402222) students of Department of Computer Science and Systems Engineering , \nAndhra University College of Engineering(A), during the period 20 21 - 2025 in the partial \nfulfilment of the requirements for the award of degree of BACHELOR OF TECHNOLOGY.  \n \n \n \n \n \n          Project Guide                                                                          Head of the Department  \nProf. K. VENKATA RAO      Prof. K. VENKATA RAO  \n      Dept of CS &SE                          Dept of CS & SE \n         AUCE,AU.                   AUCE,AU.", "metadata": {"char_count": 984, "word_count": 117, "page": 2}}, {"chunk_id": "2af33fabfb81da93_0002_67215f5f", "doc_id": "2af33fabfb81da93", "content": "DECLARATION \n \nWe, hereby declare that the project report entitled \u201cMemorize: AI Powered Knowledge \nAssistant\u201d has been done by us during the period December 202 4 \u2013 April 2025 in partial \nfulfilment of the requirement for the award of degree of Bachelor of Technology in Computer \nScience and Engineering, under the guidance of \u201cProf. K. VENKATA RAO \u201d, Associate \nProfessor, Department of Computer Science and Systems Engineering, Andhra University \nCollege of Engineering(A). We, hereby declare that this project work has not been submitted \nto any other universities/institutions for the award of any degree.  \n \n \n \n   321506402255                     PANDRINKI VENKATESH                                                                                                                                                                        \n   \n   321506402229                     NALANATI PHANISRI SAI AKSHITH", "metadata": {"char_count": 913, "word_count": 100, "page": 3}}, {"chunk_id": "2af33fabfb81da93_0003_8845e067", "doc_id": "2af33fabfb81da93", "content": "321506402229                     NALANATI PHANISRI SAI AKSHITH                           \n \n   321506402280           PRATAPARAO SUMANTH                                               \n      \n   321506402222           MAHANTHI MURALI KRISHNA                                         \n \n  \n \n \n \nPlace: Visakhapatnam \nDate:", "metadata": {"char_count": 320, "word_count": 15, "page": 3}}, {"chunk_id": "2af33fabfb81da93_0004_978308e6", "doc_id": "2af33fabfb81da93", "content": "ACKNOWLEDGEMENT \n \nWe have immense pleasure in expressing our earnest gratitude to our Project Guide Prof. K. \nVENKATA RAO, Associate Professor, Andhra University for her inspiring and scholarly \nguidance. Despite her pre -occupation with several assignments, she has been kind enough to \nspare her valuable time and gave us the necessary guidance at every stage of planning and \nconstitution of this work. We express sincere gratitude for having accorded us permission to \ntake up this project work and for helping us graciously throughout the execution of this work.  \nWe express sincere thanks to Prof. K . Venkata  Rao, Head of the Department, Computer \nScience and Systems Engineering, Andhra University College of Engineering for his keen \ninterest and providing necessary facilities for this project study. \nWe would like to  thank  Prof. D. LALITHA  BHASKARI,  Research  and  Chairperson  of  \nBOS, Department of Computer Science & Systems Engineering for the valuable guidance and", "metadata": {"char_count": 989, "word_count": 149, "page": 4}}, {"chunk_id": "2af33fabfb81da93_0005_94b83d0b", "doc_id": "2af33fabfb81da93", "content": "or this project study. \nWe would like to  thank  Prof. D. LALITHA  BHASKARI,  Research  and  Chairperson  of  \nBOS, Department of Computer Science & Systems Engineering for the valuable guidance and \nsuggestions, keen interest and thorough  encouragement extend throughout the period of project \nwork. \nWe express sincere thanks to  Prof. G. Sasibhushana Rao , Principal, Andhra University \nCollege of Engineering for his keen interest and for providing necessary facilities for this \nproject study.  \nWe express sincere gratitude to Prof. G.P. Raja Sekhar, Vice Chancellor, Andhra University \nfor his keen interest and for providing necessary facilities for this project study.  \nWe extend our sincere thanks to our academic teaching staff and nonteaching staff for their \nhelp throughout our study.", "metadata": {"char_count": 800, "word_count": 117, "page": 4}}, {"chunk_id": "2af33fabfb81da93_0006_2d4378ac", "doc_id": "2af33fabfb81da93", "content": "TABLE OF CONTENTS \nS.No.                      Title   Page No. \n1 INTRODUCTION 01-08 \n1.1 Overview 01 \n1.2 Foundational Concepts 01 \n1.3 Problem Statement 06 \n1.4 Objectives 07 \n1.5 Motivation 07 \n1.6 Scope 08 \n1.7 Methodology Overview 08 \n   \n2 LITERATURE SURVEY 9-10 \n   \n3 REQUIREMENT ANALYSIS 11-12 \n3.1 Functional Requirements 11 \n3.2 Non-Functional Requirements 12 \n3.3 System Configuration 12 \n3.3.1 Hardware Requirements 12 \n3.3.2 Software Requirements 12 \n   \n4 SYSTEM DESIGN AND METHODOLOGY 13-21 \n4.1 System Design 13 \n4.2 Existing System           14 \n4.3 Proposed System 15", "metadata": {"char_count": 586, "word_count": 80, "page": 5}}, {"chunk_id": "2af33fabfb81da93_0007_b45108fa", "doc_id": "2af33fabfb81da93", "content": "4.4 System Architecture 16 \n4.5 System Workflow 18 \n4.6 Theoretical Integration in System Design 19 \n4.6.1 RAG Pipeline Process (Module 1) 20 \n4.6.2 Podcast Workflow Design Logic (Module 2) 21 \n   \n5 IMPLEMENTATION AND TESTING 22-27 \n5.1.1 Frontend Implementation            22 \n5.1.2 Backend API Implementation 24 \n5.2 Testing 26 \n5.2.2 Testing Approach 26 \n5.2.3 Testing Outcomes 27 \n5.3.4 Overall Test Report Table 27 \n6 RESULTS 28 \n7 CONCLUSION  29 \n8 FUTURE ENHANCEMENTS 30", "metadata": {"char_count": 478, "word_count": 71, "page": 6}}, {"chunk_id": "2af33fabfb81da93_0008_6f6ddb32", "doc_id": "2af33fabfb81da93", "content": "LIST OF FIGURES \n \nS.NO FIG. NO FIGURE DESCRIPTION PG. NO \n1 1.1 Embedding Model 2 \n2 1.2 Vanilla RAG 3 \n3 1.3 Vector Embeddings 3 \n4 1.4 Hybrid Search 5 \n5 4.1 Categories of System Design 13 \n6 4.2 Module 1 System Architecture 16 \n7 4.3 Module 2 System Architecture 16", "metadata": {"char_count": 269, "word_count": 51, "page": 7}}, {"chunk_id": "2af33fabfb81da93_0009_63c81340", "doc_id": "2af33fabfb81da93", "content": "LIST OF TABLES \nS.NO TABLE NO. TABLE DESCRIPTION PG. NO \n1 1 Difference between bm35 and vector 4 \n2 5.1 Test Report Table 27", "metadata": {"char_count": 125, "word_count": 24, "page": 8}}, {"chunk_id": "2af33fabfb81da93_0010_ee50351f", "doc_id": "2af33fabfb81da93", "content": "LIST OF ABBREVATIONS \n \nAI  ARTIFICIAL INTELLIGENCE \nML MACHINE LEARNING \nLLM LARGE LANGUAGE MODEL \nRAG RETRIVAL AUGMENTED GENERATION \nNLP NATURAL LANGUAGE PROCESSING \nTTS TEXT-TO-SPEECH \nBM25 BEST MATCHING 25 \nQA QUESTION ANSWERING", "metadata": {"char_count": 232, "word_count": 30, "page": 9}}, {"chunk_id": "2af33fabfb81da93_0011_983910b0", "doc_id": "2af33fabfb81da93", "content": "ABSTRACT \nIn the age of information overload, accessing and comprehending complex academic materials \ncan be challenging for learners and researchers. A significant challenge exists in efficiently \ntransforming static documents, such as PDFs, PPTs, and Word files, int o engaging and \naccessible formats. This project addresses this challenge with an AI -powered knowledge \nassistant designed to transform these documents into interactive and spoken content. The \nsystem integrates multi -modal document ingestion, encompassing PD F, PPT, and Word \nformats, a hybrid Retrieval Augmented Generation (RAG) framework, and an AI -driven \nchapter-to-podcast generator. The RAG component employs a fusion -based approach, \ncombining dense and sparse retrieval with re -ranking, to enhance query accu racy and reduce \nhallucination when generating answers using the Gemini LLM. The podcast generator creates \nstructured audio content from chapter text, utilizing role -based summarization and a debate -", "metadata": {"char_count": 996, "word_count": 139, "page": 10}}, {"chunk_id": "2af33fabfb81da93_0012_33434d17", "doc_id": "2af33fabfb81da93", "content": "y and reduce \nhallucination when generating answers using the Gemini LLM. The podcast generator creates \nstructured audio content from chapter text, utilizing role -based summarization and a debate -\nstyle script to provide an alternative learning modality. The system is built using technologies \nsuch as LlamaIndex for data orchestration, Docling for document parsing, SQLite for \nlightweight data storage, ChromaDB for vector embeddings, Gemini LLM for content \ngeneration, Next.js for the frontend, FastAPI for th e backend, and Kokoro TTS for text -to-\nspeech synthesis. This approach aims to improve the accessibility and comprehension of \ncomplex information, catering to diverse learning styles and research needs.", "metadata": {"char_count": 722, "word_count": 104, "page": 10}}, {"chunk_id": "2af33fabfb81da93_0013_df0a916d", "doc_id": "2af33fabfb81da93", "content": "1 \n \n1. INTRODUCTION \n \n1.1 OVERVIEW \nThe project lies at the intersection of artificial intelligence, natural language processing, \nand educational technology. It addresses the growing need for efficient methods to process \nand digest the increasing volume of digital information, particularly in academic and \nresearch settings. The core idea is to develop a system that can convert static document \nformats into more dynamic and accessible formats, thereby enhancing the learning \nexperience and research productivity. This involves creating tools that not only extract \nand process the text but also transform it into interactive content and spoken formats, \ncatering to diverse learning styles and accessibility needs. \nTo elaborate, this project aims to create an AI -powered knowledge assistant that tackles \nthe difficulties faced by learners and researchers in handling large amounts of information. \nIt will use advanced AI techniques to convert static documents into more e ngaging", "metadata": {"char_count": 992, "word_count": 143, "page": 11}}, {"chunk_id": "2af33fabfb81da93_0014_dd60372e", "doc_id": "2af33fabfb81da93", "content": "sistant that tackles \nthe difficulties faced by learners and researchers in handling large amounts of information. \nIt will use advanced AI techniques to convert static documents into more e ngaging \nformats. The system will employ a hybrid Retrieval Augmented Generation (RAG) \nframework to provide accurate and reliable information and also feature an AI -driven \nchapter-to-podcast generator to offer an alternative way to consume the content. \nUltimately, this project seeks to improve the accessibility and comprehension of complex \ninformation and cater to a wider audience. \n \n1.2 Foundational Concepts \nTo fully understand the development and functionality of the AI -powered knowledge \nassistant, it is essential to explore the core theoretical and technological foundations upon \nwhich it is built. These concepts span the fields of artificial intelligence (AI), natural \nlanguage processing (NLP), machine learning (ML), large language models (LLMs),", "metadata": {"char_count": 961, "word_count": 136, "page": 11}}, {"chunk_id": "2af33fabfb81da93_0015_3fc00701", "doc_id": "2af33fabfb81da93", "content": "ological foundations upon \nwhich it is built. These concepts span the fields of artificial intelligence (AI), natural \nlanguage processing (NLP), machine learning (ML), large language models (LLMs), \nembeddings, information retrieval stra tegies, and the Retrieval -Augmented Generation \n(RAG) framework. This section elaborates on each of these components and explains how \nthey interact to create a system capable of transforming static documents into rich, \ninteractive, and voice-based knowledge formats. \n \n1.2.1 Artificial Intelligence (AI) and Machine Learning (ML) \nArtificial Intelligence (AI) refers to the capability of machines to simulate human \ncognitive functions such as learning, reasoning, and decision-making. Within the broader \nAI domain, Machine Learning (ML) is a key subfield that focuses on enabling machine s \nto learn patterns from data and improve performance over time without being explicitly \nprogrammed.", "metadata": {"char_count": 935, "word_count": 128, "page": 11}}, {"chunk_id": "2af33fabfb81da93_0016_5523628c", "doc_id": "2af33fabfb81da93", "content": "he broader \nAI domain, Machine Learning (ML) is a key subfield that focuses on enabling machine s \nto learn patterns from data and improve performance over time without being explicitly \nprogrammed. \nIn the context of this project, AI is the overarching discipline driving intelligent behavior, \nwhile ML powers the core operations such as classifying documents, generating \nsummaries, and extracting meaningful insights. Algorithms are trained on large data sets \nto understand context, relevance, and content structure, which significantly enhances the \nassistant\u2019s ability to convert raw documents into structured knowledge representations. \nFurthermore, reinforcement learning and supervised learning techniques ma y be used to \nrefine outputs, align responses with human feedback, and improve the contextual fidelity \nof generated responses.", "metadata": {"char_count": 846, "word_count": 116, "page": 11}}, {"chunk_id": "2af33fabfb81da93_0017_71de5323", "doc_id": "2af33fabfb81da93", "content": "2 \n \n1.2.2 What Are LLMs and Why Do They Need RAG? \nLarge Language Models (LLMs) like Gemini, GPT, and BERT are deep learning models \ntrained on vast amounts of textual data. They are capable of understanding and generating \nhuman-like text, answering questions, translating languages, and even summarizing \ncomplex documents. However, despite their impressive capabilities, LLMs face two \nsignificant challenges: \n\u2022 Static Knowledge Limitation: LLMs are trained on a fixed corpus of data, meaning \nthey cannot access new or real-time information beyond their training cut-off date. \n\u2022 Hallucination Problem: LLMs sometimes produce information that sounds plausible \nbut is factually incorrect, especially when handling niche or highly specific queries. \nRetrieval-Augmented Generation (RAG) addresses these issues by supplementing the \nLLM with an external knowledge base. RAG retrieves the most relevant documents or", "metadata": {"char_count": 918, "word_count": 129, "page": 12}}, {"chunk_id": "2af33fabfb81da93_0018_848ecdd3", "doc_id": "2af33fabfb81da93", "content": "niche or highly specific queries. \nRetrieval-Augmented Generation (RAG) addresses these issues by supplementing the \nLLM with an external knowledge base. RAG retrieves the most relevant documents or \ndocument fragments based on a user query and passes them into the model, enabling it to \ngenerate answers grounded in up -to-date and domain -specific information. This \nsignificantly enhances accuracy, reliability, and contextual appropriateness  of the \nresponses. Moreover, it helps the model provide traceable answers, as each generated \nresponse is tied to a known set of documents. \n \n1.2.3 Embeddings and Semantic Search \nEmbeddings are dense vector representations of text that capture the semantic meaning \nrather than just the surface -level lexical features. For example, the words \"intelligence\" \nand \"smartness\" may be different textually but are placed closely in the embedding space \ndue to their similar meanings.", "metadata": {"char_count": 929, "word_count": 133, "page": 12}}, {"chunk_id": "2af33fabfb81da93_0019_088ef627", "doc_id": "2af33fabfb81da93", "content": "t the surface -level lexical features. For example, the words \"intelligence\" \nand \"smartness\" may be different textually but are placed closely in the embedding space \ndue to their similar meanings. \nEmbeddings allow the system to perform semantic search, where the retrieval is based \nnot only on matching exact terms but also on the conceptual similarity between the query \nand the document contents. This is crucial in educational settings, where similar concepts \nmay be expressed using varied terminology. It also enables the model to better understand \nparaphrased questions and conte xtual nuances, improving the relevance of retrieved \ndocuments. \nEmbeddings are typically generated using transformer-based models like Sentence-BERT \nor Gemini\u2019s embedding service. These representations enable efficient computation and \nindexing for large-scale document search. \n \nFig 1.1 Embedding Model", "metadata": {"char_count": 897, "word_count": 125, "page": 12}}, {"chunk_id": "2af33fabfb81da93_0020_67952e27", "doc_id": "2af33fabfb81da93", "content": "3 \n \n1.2.4 Vanilla RAG: Architecture and Limitations \nThe basic version of the Retrieval -Augmented Generation (RAG) framework follows a \nthree-step pipeline: \na. The user's question is embedded into a vector. \nb. The vector is used to retrieve the top-k most semantically similar document chunks \nfrom a vector store. \nc. These retrieved chunks are provided as context to the LLM, which then generates a \nresponse based on both the query and the supporting information. \nWhile this vanilla RAG pipeline significantly improves upon stand -alone LLMs, it has \nsome limitations: \n\u2022 Keyword insensitivity : Pure semantic retrieval might overlook documents that \ncontain exact key terms, citations, or formulas that are vital in academic or technical \ndomains. \n\u2022 Lack of lexical precision : It may prioritize general relevance over highly specific \nmatches, which could lead to reduced precision. \nTo overcome these issues, more advanced retrieval techniques like hybrid search are \nincorporated.", "metadata": {"char_count": 993, "word_count": 148, "page": 13}}, {"chunk_id": "2af33fabfb81da93_0021_ab45d995", "doc_id": "2af33fabfb81da93", "content": "itize general relevance over highly specific \nmatches, which could lead to reduced precision. \nTo overcome these issues, more advanced retrieval techniques like hybrid search are \nincorporated.  \n \n \nFig 1.2 Vanilla RAG \n \n \n \nFig1.3 Vector Embeddings", "metadata": {"char_count": 251, "word_count": 33, "page": 13}}, {"chunk_id": "2af33fabfb81da93_0022_5030eb39", "doc_id": "2af33fabfb81da93", "content": "4 \n \n1.2.5 Vector Search \u2013 Semantic Similarity \nVector search operates on the embedding space, enabling the retrieval of documents based \non meaning rather than the presence of specific keywords. This is especially beneficial in \neducational applications where users might ask questions using everyday language while \nthe source documents use more formal or academic terminology. \nFor instance, a query like \u201cHow does the brain store memories?\u201d could successfully \nretrieve content that discusses \"neural encoding of long -term memory\" due to semantic \nalignment, even if no keywords overlap. \nThe system uses vector databases such as FAISS, ChromaDB, or Milvus to enable fast \napproximate nearest-neighbor search over high-dimensional embeddings. This underpins \nthe scalable and responsive search capability required for real-time interactions \n1.2.6 BM25 \u2013 Lexical Search \nBM25 (Best Matching 25) is a popular probabilistic retrieval algorithm from traditional", "metadata": {"char_count": 963, "word_count": 135, "page": 14}}, {"chunk_id": "2af33fabfb81da93_0023_ad38ec54", "doc_id": "2af33fabfb81da93", "content": "he scalable and responsive search capability required for real-time interactions \n1.2.6 BM25 \u2013 Lexical Search \nBM25 (Best Matching 25) is a popular probabilistic retrieval algorithm from traditional \ninformation retrieval. It ranks documents based on the frequency and importance of query \nterms found in each document. This type of retrieval is lexical in nature, relying strictly \non exact or partial keyword matches. \nBM25 is particularly useful in cases where documents contain technical terms, references, \nor phrases that must be matched exactly. In this project, BM25 complements semantic \nsearch by ensuring high-precision retrieval of content with exact matches, which is critical \nfor maintaining factual correctness and contextual depth. \nThe algorithm works by evaluating how frequently terms occur in a document relative to \ntheir frequency across all documents, while also considering document length", "metadata": {"char_count": 914, "word_count": 130, "page": 14}}, {"chunk_id": "2af33fabfb81da93_0024_0ef11b2f", "doc_id": "2af33fabfb81da93", "content": "orrectness and contextual depth. \nThe algorithm works by evaluating how frequently terms occur in a document relative to \ntheir frequency across all documents, while also considering document length \nnormalization. This enables it to emphasize highly relevant documents even in large \ncorpora. \n \n  \nTable 1 Difference between bm35 and vector \nFeature BM25 (Lexical Search) Vector Search (Semantic \nSearch) \nMatching Method Keyword-based Meaning-based \nStrength Exact term matching, high \nprecision \nCaptures synonyms and \ncontext, high recall \nWeakness Misses paraphrased or \nsemantically similar text \nMay miss exact keyword \nmatches", "metadata": {"char_count": 635, "word_count": 84, "page": 14}}, {"chunk_id": "2af33fabfb81da93_0025_d4dc62f9", "doc_id": "2af33fabfb81da93", "content": "5 \n \n1.2.7 Hybrid Search \u2013 Combining BM25 and Vector \nHybrid search is a retrieval technique that combines both BM25 (lexical) and vector \n(semantic) search methods. Instead of relying on a single retrieval strategy, hybrid search \nbrings together the strengths of both: \n\u2022 BM25 ensures exact keyword match and domain-specific relevance. \n\u2022 Vector search ensures conceptual coverage and paraphrase tolerance. \nThis dual approach improves both precision and recall, leading to a more balanced and \naccurate retrieval system. For this project, hybrid retrieval is essential to ensure that no \ncritical information is overlooked\u2014especially in diverse and complex academic content. \nHybrid search is implemented by combining and re -ranking the results from BM25 and \nvector search, creating a unified list of highly relevant results. This fusion approach \nincreases robustness and ensures better answer quality, even for poorly worded or \nambiguous queries. \n \n \nFig 1.4 Hybrid Search", "metadata": {"char_count": 981, "word_count": 142, "page": 15}}, {"chunk_id": "2af33fabfb81da93_0026_ef0a027e", "doc_id": "2af33fabfb81da93", "content": "a unified list of highly relevant results. This fusion approach \nincreases robustness and ensures better answer quality, even for poorly worded or \nambiguous queries. \n \n \nFig 1.4 Hybrid Search \n \n \n1.2.8 Reciprocal Rank Fusion (RRF) \u2013 Re-ranking Results \nReciprocal Rank Fusion (RRF) is a re -ranking algorithm used to merge and re -score the \nresults from multiple retrieval strategies like BM25 and vector search. Instead of simply \nchoosing results from one list or the other, RRF gives higher priority to results that appear \nnear the top of both lists. \nFor example, if a document is ranked highly in both the BM25 and vector retrieval outputs, \nRRF boosts its overall ranking. This ensures that documents with both lexical relevance \nand semantic relevance are surfaced to the top. By re -ranking in this way, th e system \nincreases the chance of presenting the most meaningful and contextually rich content to \nthe user, enhancing the overall performance of the RAG framework.", "metadata": {"char_count": 984, "word_count": 156, "page": 15}}, {"chunk_id": "2af33fabfb81da93_0027_dca6b921", "doc_id": "2af33fabfb81da93", "content": "p. By re -ranking in this way, th e system \nincreases the chance of presenting the most meaningful and contextually rich content to \nthe user, enhancing the overall performance of the RAG framework. \nThe RRF score is calculated using a formula that emphasizes early ranks, ensuring that \neven modestly placed items in both lists can rise to prominence if they are consistently \nrelevant. This makes the retrieval layer fairer, more transparent, and user-aligned.", "metadata": {"char_count": 462, "word_count": 73, "page": 15}}, {"chunk_id": "2af33fabfb81da93_0028_12ff0fc1", "doc_id": "2af33fabfb81da93", "content": "6 \n \n1.3 PROBLEM STATEMENT \nLearners and researchers today are frequently overwhelmed by the sheer volume of information \ncontained in academic papers, research reports, technical documentation, and other complex \nmaterials. Extracting relevant information efficiently from these resour ces often requires \nsignificant time and effort, especially when the content spans hundreds of pages and is \npresented in dense, technical language. Traditional methods of consuming such content\u2014such \nas manual reading, annotation, and note -taking\u2014are not onl y labor -intensive but also \nineffective for individuals with varying cognitive preferences or learning styles. These \nconventional approaches do not scale well with the increasing influx of digital information. \nMoreover, much of the content resides in static, non-interactive formats like PDFs, PowerPoint \npresentations (PPTs), and Word documents. These formats, while useful for documentation,", "metadata": {"char_count": 945, "word_count": 126, "page": 16}}, {"chunk_id": "2af33fabfb81da93_0029_5867eefd", "doc_id": "2af33fabfb81da93", "content": "information. \nMoreover, much of the content resides in static, non-interactive formats like PDFs, PowerPoint \npresentations (PPTs), and Word documents. These formats, while useful for documentation, \nare not optimized for dynamic interaction, semantic understanding, or integration into modern \nknowledge management or learning systems. This static nature of information creates a barrier \nto knowledge accessibility, reuse, and engagement. \nThis project addresses these limitations by proposing the development of a system that can \nintelligently ingest, process, and transform such documents into interactive, voice-enabled, and \ncontext-aware knowledge formats. The goal is to create a more intuit ive and engaging \nexperience for users by converting static documents into a format that supports alternative \nlearning modalities, such as podcasts, while ensuring accurate and meaningful information \nretrieval.", "metadata": {"char_count": 913, "word_count": 120, "page": 16}}, {"chunk_id": "2af33fabfb81da93_0030_f71a1638", "doc_id": "2af33fabfb81da93", "content": "7 \n \n1.4 OBJECTIVES \nThis project aims to achieve the following specific objectives: \n\u2022 Design and implement a robust system capable of ingesting documents in various \nformats, including PDF, PowerPoint, and Microsoft Word. \n\u2022 Develop and integrate a hybrid Retrieval -Augmented Generation (RAG) framework \nthat combines lexical and semantic search techniques to retrieve the most contextually \nrelevant information. \n\u2022 Create a pipeline for generating podcast -style audio content from document chapters \nusing advanced AI models, offering an alternative, auditory-based learning experience. \n\u2022 Enhance the accessibility and understandability of complex information through AI -\ndriven summarization, contextual segmentation, and voice synthesis. \n\u2022 Support a wide range of learning preferences by providing multiple ways to consume \nand interact with knowledge. \n \n \n \n1.5 MOTIVATION \nThe motivation behind this project stems from the ever -growing demand for efficient and", "metadata": {"char_count": 975, "word_count": 133, "page": 17}}, {"chunk_id": "2af33fabfb81da93_0031_e6792e19", "doc_id": "2af33fabfb81da93", "content": "arning preferences by providing multiple ways to consume \nand interact with knowledge. \n \n \n \n1.5 MOTIVATION \nThe motivation behind this project stems from the ever -growing demand for efficient and \nintelligent tools to manage the explosion of digital information. Academic professionals, \nstudents, and researchers constantly seek new methods to keep up with vast rea ding \nrequirements while maintaining a deep understanding of the content. The traditional reliance \non manual content review is not only outdated but also incompatible with the pace and scale \nof modern information consumption. \nBy building an AI-powered knowledge assistant, this project seeks to bridge the gap between \nstatic knowledge repositories and dynamic, user-centric knowledge experiences. The ability to \nautomatically convert textual documents into semantically rich, audio -based, and inte ractive \nformats empowers users to learn on the go, improve retention, and boost overall productivity.", "metadata": {"char_count": 976, "word_count": 136, "page": 17}}, {"chunk_id": "2af33fabfb81da93_0032_52e3a7aa", "doc_id": "2af33fabfb81da93", "content": "lity to \nautomatically convert textual documents into semantically rich, audio -based, and inte ractive \nformats empowers users to learn on the go, improve retention, and boost overall productivity. \nThis initiative is also driven by a commitment to inclusivity. Many individuals, including those \nwith visual impairments, attention-related challenges, or auditory learning preferences, benefit \nsignificantly from alternative content formats. The project ai ms to support such needs by \nmaking knowledge delivery more adaptable, inclusive, and personalized.", "metadata": {"char_count": 558, "word_count": 74, "page": 17}}, {"chunk_id": "2af33fabfb81da93_0033_2437dd88", "doc_id": "2af33fabfb81da93", "content": "8 \n \n1.6 SCOPE \nThe scope of this project is clearly defined to focus on the development and deployment of an \nAI-powered knowledge assistant that transforms traditional documents into dynamic learning \nassets. The primary features and functionalities to be developed include: \n\u2022 Multi-format document ingestion, supporting PDF, PPT, and Word files. \n\u2022 A hybrid RAG-based information retrieval system that combines lexical (BM25) and \nsemantic (embedding-based) methods. \n\u2022 A chapter -to-podcast generation engine utilizing LLMs and TTS (Text -to-Speech) \nsystems. \n\u2022 A user -friendly web interface for uploading documents, interacting with processed \ncontent, and listening to generated audio. \nHowever, the project explicitly excludes the following: \n\u2022 Integration with third-party learning management systems (LMS). \n\u2022 Support for document types outside of PDF, PPT, and Word formats. \n\u2022 Advanced features for audio post-processing, such as sound design, background music,", "metadata": {"char_count": 975, "word_count": 135, "page": 18}}, {"chunk_id": "2af33fabfb81da93_0034_3b961b0c", "doc_id": "2af33fabfb81da93", "content": "-party learning management systems (LMS). \n\u2022 Support for document types outside of PDF, PPT, and Word formats. \n\u2022 Advanced features for audio post-processing, such as sound design, background music, \nor professional editing. \nThis well -defined scope ensures that the project remains focused and achievable while \ndelivering significant value to its intended users. \n \n \n1.7 METHODOLOGY OVERVIEW \nThe project will be carried out in the following phases: \n\u2022 Document Ingestion and Preprocessing: Develop modules for ingesting and \npreprocessing documents in PDF, PPT, and Word formats. \n\u2022 RAG Framework Implementation: Implement a hybrid RAG framework, including \ndense and sparse retrieval mechanisms, and integrate it with the Gemini LLM. \n\u2022 Podcast Generator Development: Design and develop the AI-driven chapter-to-podcast \ngenerator, including role-based summarization and script generation. \n\u2022 System Integration and Testing: Integrate the various components into a cohesive", "metadata": {"char_count": 979, "word_count": 134, "page": 18}}, {"chunk_id": "2af33fabfb81da93_0035_ca7109a0", "doc_id": "2af33fabfb81da93", "content": "and develop the AI-driven chapter-to-podcast \ngenerator, including role-based summarization and script generation. \n\u2022 System Integration and Testing: Integrate the various components into a cohesive \nsystem and conduct thorough testing. \n\u2022 User Interface Development: Develop a user -friendly interface for accessing and \ninteracting with the processed content.", "metadata": {"char_count": 361, "word_count": 46, "page": 18}}, {"chunk_id": "2af33fabfb81da93_0036_81e3b295", "doc_id": "2af33fabfb81da93", "content": "9 \n \n2. LITERATURE SURVEY \n2.1 Review of Existing Systems: \nA number of existing systems and technologies address various aspects of the problem of \ninformation access and processing. Here's an overview of some key areas: \n\u2022 Document Management Systems: Systems like Adobe Acrobat, Microsoft SharePoint, and \nGoogle Workspace provide functionalities for storing, organizing, and searching documents. \nHowever, they primarily focus on keyword -based search and lack the advanced AI -powered \nsemantic understanding and content transformation capabilities of this project. \n\u2022 Information Retrieval Systems: Search engines like Google Search and Bing utilize \nsophisticated algorithms, including BM25 and vector -based methods, for retrieving relevant \ninformation from vast web resources. However, they are designed for web -scale data and do \nnot address the specific challenges of processing and transforming structured documents like \nPDFs, PPTs, and Word files into alternative formats.", "metadata": {"char_count": 988, "word_count": 136, "page": 19}}, {"chunk_id": "2af33fabfb81da93_0037_5d2c16e5", "doc_id": "2af33fabfb81da93", "content": "ever, they are designed for web -scale data and do \nnot address the specific challenges of processing and transforming structured documents like \nPDFs, PPTs, and Word files into alternative formats. \n\u2022 Large Language Models (LLMs): Models like OpenAI's GPT series and Google's Gemini \nhave demonstrated remarkable capabilities in natural language understanding, generation, and \nsummarization. These models can be used to enhance information retrieval and cont ent \ntransformation. This project leverages the Gemini LLM to generate accurate and contextually \nrelevant answers and to create engaging podcast scripts. \n\u2022 Text-to-Speech (TTS) Systems: Services like Amazon Polly, Google Text -to-Speech, and \nMicrosoft Azure Text to Speech can convert text into spoken audio. While these systems \nprovide the basic functionality for converting text to speech, this project aims to go further by \ngenerating structured and engaging audio content with role -based summarization and debate-", "metadata": {"char_count": 984, "word_count": 140, "page": 19}}, {"chunk_id": "2af33fabfb81da93_0038_74305545", "doc_id": "2af33fabfb81da93", "content": "systems \nprovide the basic functionality for converting text to speech, this project aims to go further by \ngenerating structured and engaging audio content with role -based summarization and debate-\nstyle scripts. \nWhile these existing systems offer valuable functionalities, this project aims to integrate and \nextend these capabilities to create a more comprehensive and intelligent knowledge assistant. \nIt combines the strengths of multi-modal document processing, advanced RAG techniques, and \nAI-driven content transformation to provide a more effective solution for accessing and \nunderstanding complex information.", "metadata": {"char_count": 623, "word_count": 83, "page": 19}}, {"chunk_id": "2af33fabfb81da93_0039_882d9dc5", "doc_id": "2af33fabfb81da93", "content": "10 \n \n2.2 Identified Gaps and Innovations: \nThis project addresses several gaps in existing systems and introduces innovative approaches: \n\u2022 Integration of Multi-Modal Documents: Unlike many existing systems that focus on a \nsingle document format or web -based content, this project integrates multi -modal \ndocument ingestion, including PDF, PPT, and Word files. This enables users to process \na wider range of document types without needing to convert them manually. \n\u2022 Hybrid RAG Framework with Re -ranking: The project employs a hybrid RAG \nframework that combines BM25, vector search, and Reciprocal Rank Fusion (RRF). \nThis approach overcomes the limitations of individual retrieval methods and improves \nthe accuracy and robu stness of information retrieval. The re -ranking step further \nenhances the quality of the retrieved information, ensuring that the most relevant \ncontent is presented to the user. \n\u2022 AI-Driven Podcast Generation: The project introduces an innovative approach to", "metadata": {"char_count": 996, "word_count": 145, "page": 20}}, {"chunk_id": "2af33fabfb81da93_0040_610963d8", "doc_id": "2af33fabfb81da93", "content": "nhances the quality of the retrieved information, ensuring that the most relevant \ncontent is presented to the user. \n\u2022 AI-Driven Podcast Generation: The project introduces an innovative approach to \ncontent transformation by generating structured, engaging podcasts from chapter text. \nThis goes beyond simple text -to-speech conversion by incorporating role -based \nsummarization and debate-style scripts. This provides an alternative learning modality \nthat can improve accessibility and engagement, particularly for auditory learners. \n\u2022 End-to-End System for Knowledge Access: By combining these innovations, the \nproject creates an end -to-end system that streamlines the process of accessing and \nunderstanding complex information. The system takes static documents as input and \nprovides users with both interactive Q&A capabilities and dynamic audio content, all \npowered by advanced AI.", "metadata": {"char_count": 896, "word_count": 121, "page": 20}}, {"chunk_id": "2af33fabfb81da93_0041_f30a1b50", "doc_id": "2af33fabfb81da93", "content": "11 \n \n3. REQUIREMENTS ANALYSIS \nSoftware Requirement Specification (SRS) is a complete specification and description of \nsoftware requirements that must be fulfilled to develop the software system successfully. It \ncomprises user requirements for a system and detailed specifications of the system \nrequirements. This report lays a foundation for software engineering activities and is \nconstructed when requirements are elicited and analyzed. \nA Software Requirements Spe cification (SRS) document for the Memorize outlines the \ndetailed requirements and specifications for the development of the system. \n3.1. FUNCTIONAL REQUIREMENTS: \nThe system allows users to register, create knowledge spaces, and upload documents in PDF, \nPPT, or Word formats. It automatically extracts text using Docling and embeds it into a vector \nstore for efficient querying. A hybrid RAG pipeline (dense + sparse retrieval) handles real-time", "metadata": {"char_count": 922, "word_count": 128, "page": 21}}, {"chunk_id": "2af33fabfb81da93_0042_2d389f4c", "doc_id": "2af33fabfb81da93", "content": "PDF, \nPPT, or Word formats. It automatically extracts text using Docling and embeds it into a vector \nstore for efficient querying. A hybrid RAG pipeline (dense + sparse retrieval) handles real-time \nqueries, with results refined through RAG fusion and Reciprocal Rank Fusion. Users can \ninteract with an AI chatbot powered by Gemini LLM and convert selected chapters or topics \ninto podcasts using Kokoro TTS, which are then playable via an integrated audio player \n\u2022 Document Upload: Users can upload a variety of document formats, including PDF \nfiles, PowerPoint presentations (PPT), and Microsoft Word documents. The upload \nmechanism supports drag -and-drop functionality and validates file types before \nprocessing. \n\u2022 Text Extraction:  Upon upload, documents are automatically parsed to extract \nstructured, clean textual content. This is achieved using Docling, a robust tool for \nintelligent parsing and formatting of text across diverse document types. Extracted", "metadata": {"char_count": 973, "word_count": 142, "page": 21}}, {"chunk_id": "2af33fabfb81da93_0043_698cd97c", "doc_id": "2af33fabfb81da93", "content": "matically parsed to extract \nstructured, clean textual content. This is achieved using Docling, a robust tool for \nintelligent parsing and formatting of text across diverse document types. Extracted \ncontent is preprocessed for tokenization and chunking. \n\u2022 Ingestion Trigger: Users can initiate the ingestion process manually after uploading. \nThis process converts document chunks into vector embeddings and stores them in a \nvector database. It ensures fine-tuned semantic representation and indexing. \n\u2022 Hybrid Search & Querying:  The system supports real -time information retrieval \nusing a hybrid search approach. It combines dense vector -based semantic search with \nsparse lexical retrieval (BM25) to maximize both contextual understanding and \nkeyword precision. \n\u2022 RAG Fusion & Ranking: For each query, similar sub-queries are generated to expand \nthe search context (RAG fusion). The results are consolidated and reranked using", "metadata": {"char_count": 939, "word_count": 132, "page": 21}}, {"chunk_id": "2af33fabfb81da93_0044_a0a7dc7f", "doc_id": "2af33fabfb81da93", "content": "tanding and \nkeyword precision. \n\u2022 RAG Fusion & Ranking: For each query, similar sub-queries are generated to expand \nthe search context (RAG fusion). The results are consolidated and reranked using \nReciprocal Rank Fusion (RRF), ensuring the most relevant content surfaces at the top. \n\u2022 Conversational AI Response:  Using the Gemini large language model (LLM), the \nsystem generates detailed, context -aware answers to user queries. The responses are \ntailored to the document corpus and maintain high factual integrity. \n\u2022 Chapter Selection & Podcast Generation: Users can select entire chapters or specific \nsections and optionally define focus topics. The system then generates podcast -ready \nscripts that summarize and explain the content intelligently. \n\u2022 TTS Synthesis:  Using Kokoro TTS, the system synthesizes the AI -generated script \ninto high-quality, natural-sounding audio. The voice output is expressive and listener -\nfriendly.", "metadata": {"char_count": 945, "word_count": 136, "page": 21}}, {"chunk_id": "2af33fabfb81da93_0045_785ac052", "doc_id": "2af33fabfb81da93", "content": "ntelligently. \n\u2022 TTS Synthesis:  Using Kokoro TTS, the system synthesizes the AI -generated script \ninto high-quality, natural-sounding audio. The voice output is expressive and listener -\nfriendly. \n\u2022 Audio Playback:  An integrated web -based audio player allows users to listen to \ngenerated podcasts directly in the browser. Playback controls include pause, skip, and \nvolume adjustments.", "metadata": {"char_count": 391, "word_count": 55, "page": 21}}, {"chunk_id": "2af33fabfb81da93_0046_4fe3eba8", "doc_id": "2af33fabfb81da93", "content": "12 \n \n3.2. NON-FUNCTIONAL REQUIREMENTS \nThe system ensures high performance through GPU -accelerated processing and supports \nmultiple users with scalable ingestion. ChromaDB and SQLite provide persistent storage for \ndocuments and audio. The Next.js frontend offers a smooth, user-friendly interface, and strong \nsecurity measures protect file uploads and access. \n\u2022 Performance: The system is optimized for speed and responsiveness, with GPU-\naccelerated embedding for fast ingestion and low-latency TTS for near real-time \naudio generation. \n\u2022 Scalability: Designed with a modular architecture, the system supports concurrent \nusers and scalable ingestion pipelines for handling multiple documents and \ninteractions simultaneously. \n\u2022 Reliability: Persistent storage of document embeddings and generated audio files is \nensured via ChromaDB and SQLite, minimizing data loss and enhancing fault \ntolerance. \n\u2022 Usability: The frontend, developed with Next.js, offers a modern and intuitive user", "metadata": {"char_count": 995, "word_count": 132, "page": 22}}, {"chunk_id": "2af33fabfb81da93_0047_e2b237a2", "doc_id": "2af33fabfb81da93", "content": "generated audio files is \nensured via ChromaDB and SQLite, minimizing data loss and enhancing fault \ntolerance. \n\u2022 Usability: The frontend, developed with Next.js, offers a modern and intuitive user \ninterface. Users benefit from clear navigation, visual feedback, and seamless \ntransitions. \n\u2022 Security: All file uploads are handled through secure endpoints with proper \nauthentication and access control. Permissions ensure that user data and documents \nremain private and protected.. \n3.3. SYSTEM REQUIREMENTS \n3.3.1Hardware Requirements \n\u2022 Development/Server Machine: \no Processor: Quad-core CPU or higher \no RAM: Minimum 16 GB \no GPU: NVIDIA GPU with CUDA support (e.g., RTX 3060 or better) for \nembeddings and TTS \no Storage: SSD with at least 50 GB free space for document and audio storage \n3.3.2Software Requirements \n\u2022 Backend: Python 3.10+, FastAPI, CUDA toolkit \n\u2022 Frontend: Next.js 14, Tailwind CSS \n\u2022 AI/ML Libraries: LlamaIndex, HuggingFace Transformers, ChromaDB, Docling,", "metadata": {"char_count": 988, "word_count": 140, "page": 22}}, {"chunk_id": "2af33fabfb81da93_0048_a166b590", "doc_id": "2af33fabfb81da93", "content": "storage \n3.3.2Software Requirements \n\u2022 Backend: Python 3.10+, FastAPI, CUDA toolkit \n\u2022 Frontend: Next.js 14, Tailwind CSS \n\u2022 AI/ML Libraries: LlamaIndex, HuggingFace Transformers, ChromaDB, Docling, \nGemini API \n\u2022 TTS Engine: Kokoro TTS \n\u2022 Database: SQLite (for metadata), file system (for media)", "metadata": {"char_count": 296, "word_count": 40, "page": 22}}, {"chunk_id": "2af33fabfb81da93_0049_50c6f691", "doc_id": "2af33fabfb81da93", "content": "13 \n \n4. SYSTEM DESIGN \n4.1. SYSTEM DESIGN \nSystem design refers to the process of defining a system\u2019s architecture, components, modules, \nand data flow to ensure it meets functional and non-functional requirements effectively. It \nprovides a structured approach to software development, ensuring scalability, maintainability, \nand efficiency. A well-defined system design helps in understanding how different components \ninteract, how data is processed, and how users will interact with the system. \nTypes of System Design \nSystem design is broadly categorized into two main types: \n1. High-Level Design (HLD): Also known as architectural design, it defines the system's \noverall structure, including major components, their relationships, and how they \ninteract. It provides a macro-level view of the system and focuses on aspects like data \nflow, module division, and technology choices. It provides an abstract representation", "metadata": {"char_count": 929, "word_count": 130, "page": 23}}, {"chunk_id": "2af33fabfb81da93_0050_9c390ac7", "doc_id": "2af33fabfb81da93", "content": "ionships, and how they \ninteract. It provides a macro-level view of the system and focuses on aspects like data \nflow, module division, and technology choices. It provides an abstract representation \nof the system, helpi ng stakeholders understand its functionality without delving into \nimplementation details. \n2. Low-Level Design (LLD): Low-Level Design (LLD) translates the high-level system \narchitecture into detailed specifications for each module, defining the logic, database \nschemas (if used), data structures, API endpoints, and internal workflows. It focuses on \nclass diagrams, seque nce diagrams, and algorithm design to ensure efficient \nimplementation. LLD serves as a blueprint for developers, ensuring consistency and \nclarity in code development. \n \nFig 4.1. Categories of System Design", "metadata": {"char_count": 806, "word_count": 111, "page": 23}}, {"chunk_id": "2af33fabfb81da93_0051_9fcdc74f", "doc_id": "2af33fabfb81da93", "content": "14 \n \n4.2. EXISTING SYSTEM \nTraditional methods of accessing, processing, and understanding complex documents typically \ninvolve a combination of manual effort and basic digital tools. These conventional approaches \noften include: \n\u2022 Manually reading through lengthy documents to extract relevant information. \n\u2022 Taking handwritten or digital notes while reading, which is both time -consuming and \ncognitively demanding. \n\u2022 Performing keyword-based searches within digital files to locate specific terms or \nsections. \n\u2022 Using general-purpose text-to-speech (TTS) applications to convert written text into \nspoken audio. \nWhile these methods have been widely used in academic, professional, and research settings, \nthey suffer from several significant drawbacks: \n\u2022 Inefficiency and time consumption : Users often spend hours reading, highlighting, \nand summarizing content, especially when dealing with dense or voluminous \ndocuments.", "metadata": {"char_count": 936, "word_count": 123, "page": 24}}, {"chunk_id": "2af33fabfb81da93_0052_77898e2a", "doc_id": "2af33fabfb81da93", "content": "al significant drawbacks: \n\u2022 Inefficiency and time consumption : Users often spend hours reading, highlighting, \nand summarizing content, especially when dealing with dense or voluminous \ndocuments. \n\u2022 Cognitive load : These methods require the user to manually identify, extract, and \nsynthesize key points, which can lead to information overload and reduced retention. \n\u2022 Inflexibility of keyword search : Traditional search functions depend on exact \nkeyword matches, which means that semantically relevant content using alternative \nterminology may go unnoticed. \n\u2022 Lack of engagement in TTS output: Basic TTS tools provide a linear and monotonous \nreading of content without context-aware structuring, emotional inflection, or dynamic \npresentation, making them less effective for comprehension and long-term learning. \nIn contrast, this project proposes a transformative solution through the development of an AI -", "metadata": {"char_count": 920, "word_count": 128, "page": 24}}, {"chunk_id": "2af33fabfb81da93_0053_0ed0f723", "doc_id": "2af33fabfb81da93", "content": "ection, or dynamic \npresentation, making them less effective for comprehension and long-term learning. \nIn contrast, this project proposes a transformative solution through the development of an AI -\npowered knowledge assistant. The system aims to automate and enhance the way users interact \nwith documents by converting static text into dynamic, interactive, an d multimodal formats. \nWith features like semantic search, intelligent summarization, and natural -sounding podcast \ngeneration, the assistant addresses the core limitations of existing systems, offering a more \nefficient, user-friendly, and engaging way to access and understand complex information.", "metadata": {"char_count": 664, "word_count": 90, "page": 24}}, {"chunk_id": "2af33fabfb81da93_0054_16534adf", "doc_id": "2af33fabfb81da93", "content": "15 \n \n4.3. PROPOSED SOLUTION \nThe proposed system is an end -to-end AI-powered knowledge assistant that transforms static \ndocuments into interactive and audio-based content for enhanced accessibility, comprehension, \nand engagement. It is designed with modularity and scalability in mind, integrating multiple \nAI components into a cohesive workflow. \nThe system accepts documents in PDF, PPT, and Word formats and processes them through a \nhybrid RAG (Retrieval Augmented Generation) pipeline for contextual Q&A, while also \ngenerating structured podcast -style audio summaries. It provides a user -friendly fro ntend \nwhere users can upload documents, search for information conversationally, and listen to \nchapter-wise audio summaries. \nKey functionalities include: \n\u2022 Multi-format Document Ingestion: Seamlessly upload and parse content from PDFs, \nPPTs, and Word files. \n\u2022 Contextual Information Retrieval : Query documents using a hybrid RAG pipeline", "metadata": {"char_count": 958, "word_count": 130, "page": 25}}, {"chunk_id": "2af33fabfb81da93_0055_c68ee7a6", "doc_id": "2af33fabfb81da93", "content": "s include: \n\u2022 Multi-format Document Ingestion: Seamlessly upload and parse content from PDFs, \nPPTs, and Word files. \n\u2022 Contextual Information Retrieval : Query documents using a hybrid RAG pipeline \nthat combines dense and sparse retrieval with re-ranking. \n\u2022 Conversational Response Generation: Generate human-like responses using Gemini \nLLM grounded in retrieved content. \n\u2022 Podcast Generator: Convert chapters into debate-style podcast scripts, synthesized to \nnatural audio via Kokoro TTS. \n\u2022 Interactive UI : Provide an intuitive web interface for document management, \nquerying, and audio playback. \nThe system prioritizes performance, reliability, and user accessibility, with GPU acceleration \nfor embedding and TTS, and persistent storage via ChromaDB and SQLite.", "metadata": {"char_count": 774, "word_count": 105, "page": 25}}, {"chunk_id": "2af33fabfb81da93_0056_9346db85", "doc_id": "2af33fabfb81da93", "content": "16 \n \n4.4. SYSTEM ARCHITECTURE \nThe architecture of the proposed system is composed of four major layers: Frontend Layer, \nBackend Layer, Processing Layer, and Data Storage Layer. These layers communicate over \nRESTful APIs and real-time channels, ensuring modularity and maintainability. \n \n \n \nFig 4.2 Module 1 System Architecture \n \n \nFig 4.3 Module 2 System Architecture", "metadata": {"char_count": 374, "word_count": 51, "page": 26}}, {"chunk_id": "2af33fabfb81da93_0057_121e880a", "doc_id": "2af33fabfb81da93", "content": "17 \n \n1. Frontend Layer (Client Interface) \n\u2022 Technology: Built using Next.js and Tailwind CSS \n\u2022 Functions: \no User registration and authentication \no Document upload UI \no Conversational chat interface \no Chapter selection and podcast playback \no Visual feedback on query results \n2. Backend Layer (API & Orchestration) \n\u2022 Technology: Python with FastAPI \n\u2022 Responsibilities: \no Handle API endpoints for document upload, ingestion, querying, and podcast \ngeneration \no Coordinate between document parser, vector store, Gemini LLM, and TTS \nengine \no Manage user sessions, permissions, and task queues \n \n3. Processing Layer (AI & NLP Pipeline) \n\u2022 Components: \no Document Parser: Docling extracts structured text and metadata \no Embedding Engine: Generates dense embeddings using HuggingFace models \no Hybrid RAG Module: \n\u25aa Sparse retrieval via BM25 \n\u25aa Dense retrieval via vector search (ChromaDB) \n\u25aa Re-ranking with Reciprocal Rank Fusion", "metadata": {"char_count": 940, "word_count": 135, "page": 27}}, {"chunk_id": "2af33fabfb81da93_0058_22c72bc5", "doc_id": "2af33fabfb81da93", "content": "gine: Generates dense embeddings using HuggingFace models \no Hybrid RAG Module: \n\u25aa Sparse retrieval via BM25 \n\u25aa Dense retrieval via vector search (ChromaDB) \n\u25aa Re-ranking with Reciprocal Rank Fusion \no LLM Interaction: Gemini API for summarization, Q&A, and podcast script \ngeneration \no TTS Engine: Kokoro TTS converts structured scripts into high-quality audio", "metadata": {"char_count": 362, "word_count": 52, "page": 27}}, {"chunk_id": "2af33fabfb81da93_0059_bee2723c", "doc_id": "2af33fabfb81da93", "content": "18 \n \n4. Data Storage Layer \n\u2022 Document Storage: \no Parsed documents stored in SQLite for lightweight querying \no Embedded vectors stored in ChromaDB for semantic search \n\u2022 Audio Storage: \no Generated podcast files stored in local or cloud storage \no Metadata indexed for playback and retrieval \n \n4.5. SYSTEM WORKFLOW \n\u2022 Upload: User uploads a document via the frontend. \n\u2022 Ingestion: Document is parsed, and content is embedded into ChromaDB. \n\u2022 Querying: User inputs a question. The system retrieves relevant context via hybrid \nRAG and generates an answer using Gemini LLM. \n\u2022 Podcast Generation: User selects a chapter and optionally a focus topic. The system \nsummarizes and converts it into a scripted audio using Kokoro TTS. \n\u2022 Playback: The user can stream or download the generated podcast.", "metadata": {"char_count": 800, "word_count": 126, "page": 28}}, {"chunk_id": "2af33fabfb81da93_0060_59e1cb30", "doc_id": "2af33fabfb81da93", "content": "19 \n \n4.6 Theoretical Integration in System Design \nAt its core, the system implements a Hybrid Retrieval -Augmented Generation (RAG)  \narchitecture that merges semantic understanding with traditional information retrieval. \n\u2022 Dense Embeddings capture the contextual meaning of text using transformer -based \nmodels. These allow for semantic similarity search. \n\u2022 Sparse Vectors (BM25)  complement this with keyword -based relevance scoring, \nimproving retrieval on keyword-heavy queries. \n\u2022 Hybrid Retrieval  uses both to get broader, more accurate coverage of possible \nanswers. \n\u2022 RAG Fusion  enhances recall by generating multiple semantically -relevant query \nvariations and retrieving top results for each. \n\u2022 Reciprocal Rank Fusion (RRF)  is used to combine the results from hybrid searches \ninto a unified, reranked list that prioritizes documents most relevant across variations. \n\u2022 Final Generation  involves passing this high -quality, context -rich information to a", "metadata": {"char_count": 977, "word_count": 134, "page": 29}}, {"chunk_id": "2af33fabfb81da93_0061_bf0c3372", "doc_id": "2af33fabfb81da93", "content": "om hybrid searches \ninto a unified, reranked list that prioritizes documents most relevant across variations. \n\u2022 Final Generation  involves passing this high -quality, context -rich information to a \npowerful LLM (Gemini) to generate low-hallucination, grounded responses. \nThis theoretical layering of dense + sparse + rerankers makes the system robust across diverse \nquery types, including vague, technical, or high-level questions.", "metadata": {"char_count": 435, "word_count": 59, "page": 29}}, {"chunk_id": "2af33fabfb81da93_0062_9c933013", "doc_id": "2af33fabfb81da93", "content": "20 \n \n4.6.1 RAG Pipeline Process (Module 1) \n1. Document Ingestion & Parsing: \no When a user uploads PDFs, PPTs, or Word files into a \"space,\" the backend \ncreates a corresponding folder and inserts a record in the spaces table (SQLite). \no Uploaded documents are parsed using Docling, which extracts raw textual \ncontent. \n2. Chunking & Storage: \no Extracted text is split into manageable chunks. \no Each chunk is stored in the documents table in SQLite for later processing. \n3. Ingestion Trigger: \no When the user clicks the \u201cIngest\u201d button, the backend /ingestion route is \nactivated. \n4. Embedding: \no Unembedded chunks are converted to dense embeddings using: \nHuggingFaceEmbedding(model_name=\"BAAI/bge-base-en-v1.5\", device=\"cuda\") \no Dense embeddings are stored in ChromaDB. \no BM25 sparse vectors are generated and persisted to disk. \n5. Indexing: \no An index (dense + sparse) is created per space to support efficient retrieval. \n6. Query Workflow (RAG Fusion):", "metadata": {"char_count": 971, "word_count": 146, "page": 30}}, {"chunk_id": "2af33fabfb81da93_0063_a8198f56", "doc_id": "2af33fabfb81da93", "content": "ChromaDB. \no BM25 sparse vectors are generated and persisted to disk. \n5. Indexing: \no An index (dense + sparse) is created per space to support efficient retrieval. \n6. Query Workflow (RAG Fusion): \no When a user sends a query, it hits the /query route. \no The relevant index is dynamically loaded based on space. \no RAG Fusion generates multiple paraphrased queries. \no Each variant is run through: \n\u25aa Dense vector search (ChromaDB) \n\u25aa Sparse BM25 search (disk) \no Top 7 nodes per query are retrieved. \no Reciprocal Rank Fusion (RRF) consolidates and reranks results. \no Top-ranked nodes are passed with the query to Gemini LLM, which generates \na high-quality, context-aware answer.", "metadata": {"char_count": 685, "word_count": 111, "page": 30}}, {"chunk_id": "2af33fabfb81da93_0064_e48eb00b", "doc_id": "2af33fabfb81da93", "content": "21 \n \n4.6.2 Podcast Workflow Design Logic (Module 2) \n1. Input & Preprocessing: \no User selects a chapter (optional: provides focus topic). \no Text (up to 2M tokens) is isolated from document. \no Context buffers are added to ensure coherence. \n2. Outline Generation: \no Gemini LLM generates a structured podcast outline: \n\u25aa Sections include: Intro, Key Concepts, Summary, etc. \n3. Role-Based Summarization: \no Two summaries are produced: \n\u25aa Expert: Technical explanation \n\u25aa Novice: Simplified version \n4. Dialogue Generation: \no AI simulates a debate using: \n\u25aa Outline \n\u25aa Both summaries \no Produces: Scripted dialogue (Expert vs. Novice) \n5. Text-to-Speech Synthesis: \no Dialogue script is passed to Kokoro TTS. \no Each turn is converted to speech. \no Clips are concatenated and saved to disk. \no Final audio path is stored in SQLite.", "metadata": {"char_count": 834, "word_count": 129, "page": 31}}, {"chunk_id": "2af33fabfb81da93_0065_5eacc15b", "doc_id": "2af33fabfb81da93", "content": "22 \n \n5. IMPLEMENTATION AND TESTING \n5.1. IMPLEMENTATION \nFrontend Implementation \nThe frontend of the AI-powered knowledge assistant is built using Next.js, offering a fast and \nscalable framework for building web applications. It provides both server-side rendering (SSR) \nand static site generation (SSG), allowing for efficient data-fetching and routing mechanisms. \n Technologies Used \n\u2022 Next.js 14: For routing, page-based architecture, SSR/SSG. \n\u2022 React Query: Manages data fetching and caching across the app. \n\u2022 Tailwind CSS : Provides a utility -first CSS framework for rapid and responsive UI \ndevelopment. \n\u2022 Axios: Handles HTTP requests to the FastAPI backend. \n\u2022 Framer Motion: Adds smooth transitions and animations. \n\u2022 React Dropzone: For file uploads. \n\u2022 ShadCN/UI Components: Used for modern UI elements like modals, toasts, and cards. \n Core Features \n1. User Spaces UI \no Users can create \"spaces\" where documents are uploaded and processed.", "metadata": {"char_count": 961, "word_count": 138, "page": 32}}, {"chunk_id": "2af33fabfb81da93_0066_0f7afdd9", "doc_id": "2af33fabfb81da93", "content": "oads. \n\u2022 ShadCN/UI Components: Used for modern UI elements like modals, toasts, and cards. \n Core Features \n1. User Spaces UI \no Users can create \"spaces\" where documents are uploaded and processed. \no Each space is represented by a card showing its name and status. \no State updates with React Query ensure real-time sync with the backend. \n2. Document Upload Interface \no Users can upload PDF, PPT, or DOC files using a drag-and-drop interface. \no Upload status and feedback are shown using toast notifications. \no Files are sent to the /upload/ backend endpoint. \n3. Ingest Button & Status Monitoring \no Once files are uploaded, the \"Ingest\" button appears. \no Triggers the /ingestion/ route to begin the chunking and embedding pipeline. \no Progress indicators guide the user through each backend operation.", "metadata": {"char_count": 810, "word_count": 129, "page": 32}}, {"chunk_id": "2af33fabfb81da93_0067_292f704b", "doc_id": "2af33fabfb81da93", "content": "23 \n \n4. Query Interface \no Users can input natural language questions. \no Responses from Gemini are shown below the query box. \no Past queries are optionally stored per space for review. \n5. Podcast Generator UI \no For Module 2, users can select a document, choose a chapter, and optionally \nadd a focus topic. \no Multi-step UI walks through: \n\u25aa Preprocessing \n\u25aa Outline generation \n\u25aa Role-based summarization \n\u25aa Dialogue review \n\u25aa Audio generation \no Final podcast audio is streamed from the stored path. \n \nFront-end output:", "metadata": {"char_count": 527, "word_count": 84, "page": 33}}, {"chunk_id": "2af33fabfb81da93_0068_deb429af", "doc_id": "2af33fabfb81da93", "content": "24 \n \nBackend API Implementation \nOverview \nThe backend is developed using FastAPI, providing an efficient and scalable REST API for \nhandling document uploads, query processing, and podcast generation. The backend interacts \nwith various systems like SQLite for data storage, ChromaDB for vector embeddings, Gemini \nLLM for answer generation, and Kokoro TTS for podcast synthesis. \nAPI Endpoints \n1. spaces.py \no Endpoint: /createspace/ \n\u25aa Method: POST \n\u25aa Input: \n\u25aa space_name: The name of the space to be created (required). \n\u25aa db: Database session (Dependency). \n\u25aa Response: \n\u25aa JSON object with message, space_id, and space_name. \no Endpoint: /spaces/ \n\u25aa Method: GET \n\u25aa Input: \n\u25aa db: Database session (Dependency). \n\u25aa Response: \n\u25aa JSON array of all available spaces. \n2. upload.py \no Endpoint: /upload/ \n\u25aa Method: POST \n\u25aa Input: \n\u25aa space_id: The ID of the space to upload files to (required). \n\u25aa files: List of files to be uploaded (required). \n\u25aa db: Database session (Dependency). \n\u25aa Response:", "metadata": {"char_count": 997, "word_count": 151, "page": 34}}, {"chunk_id": "2af33fabfb81da93_0069_307eb9e6", "doc_id": "2af33fabfb81da93", "content": "pload/ \n\u25aa Method: POST \n\u25aa Input: \n\u25aa space_id: The ID of the space to upload files to (required). \n\u25aa files: List of files to be uploaded (required). \n\u25aa db: Database session (Dependency). \n\u25aa Response: \n\u25aa JSON object with details of the uploaded files and extracted text.", "metadata": {"char_count": 268, "word_count": 46, "page": 34}}, {"chunk_id": "2af33fabfb81da93_0070_aa84666f", "doc_id": "2af33fabfb81da93", "content": "25 \n \n3. ingestion.py \no Endpoint: /ingestion/{space_id} \n\u25aa Method: POST \n\u25aa Input: \n\u25aa space_id: The ID of the space for which documents are to be \ningested (required). \n\u25aa db: Database session (Dependency). \n\u25aa Response: \n\u25aa JSON object with details of the ingestion process. \n4. query.py \no Endpoint: /query/{space_id} \n\u25aa Method: POST \n\u25aa Input: \n\u25aa space_id: The ID of the space to query (required). \n\u25aa request: JSON object with query_text (required). \n\u25aa Response: \n\u25aa JSON object with query results. \n5. podcast.py \no Endpoint: /podcast/{space_id} \n\u25aa Method: POST \n\u25aa Input: \n\u25aa space_id: The ID of the space for which the podcast is to be \ngenerated (required). \n\u25aa focus_topic: Optional, the focus topic for the podcast (Query \nparameter). \n\u25aa db: Database session (Dependency). \n\u25aa Response: \n\u25aa JSON object with details of the generated podcast.", "metadata": {"char_count": 840, "word_count": 130, "page": 35}}, {"chunk_id": "2af33fabfb81da93_0071_14447ac1", "doc_id": "2af33fabfb81da93", "content": "26 \n \n5.3. TESTING \nTesting is a fundamental aspect of the development lifecycle of our AI-powered knowledge \nassistant and podcast generator system. It ensures reliability, functional correctness, and \nseamless user experience across the document-driven RAG pipeline and audio generation \nworkflows. This section outlines the structured testing strategies and methodologies \nemployed. \n5.3.1. Testing Approach \nA layered and iterative testing strategy was followed to detect and resolve issues early. This \napproach helped ensure that each module \u2014from document ingestion to hybrid retrieval and \naudio synthesis\u2014was validated both in isolation and within the integrated system. We aimed \nto maximize accuracy, minimize latency, and validate user flows end-to-end. \n5.3.2. Types of Testing \n\u2022 Unit Testing: Each major function, such as document parsing (Docling), text chunking, \nembedding generation, and audio synthesis, was tested independently. Python -based", "metadata": {"char_count": 963, "word_count": 130, "page": 36}}, {"chunk_id": "2af33fabfb81da93_0072_c85fc328", "doc_id": "2af33fabfb81da93", "content": "5.3.2. Types of Testing \n\u2022 Unit Testing: Each major function, such as document parsing (Docling), text chunking, \nembedding generation, and audio synthesis, was tested independently. Python -based \nunit testing frameworks (e.g., PyTest) were used to validate edge cases and ensure \npredictable outpu ts. Example cases included empty documents, corrupted PDFs, or \ninvalid text encodings. \n\u2022 Integration Testing : Focused on ensuring correct interaction between document \nextraction, embedding generation, ChromaDB storage, and BM25 indexing. For \ninstance, we verified that embedded chunks were stored and retrieved correctly and that \nqueries propagated cleanly through the RAG pipeline. \n\u2022 System Testing : The entire platform \u2014from the Next.js frontend to the FastAPI \nbackend\u2014was tested as a cohesive system. Full upload -to-response and chapter -to-\naudio pipelines were simulated to confirm end-to-end stability and correctness.", "metadata": {"char_count": 934, "word_count": 130, "page": 36}}, {"chunk_id": "2af33fabfb81da93_0073_afb14ac6", "doc_id": "2af33fabfb81da93", "content": "he Next.js frontend to the FastAPI \nbackend\u2014was tested as a cohesive system. Full upload -to-response and chapter -to-\naudio pipelines were simulated to confirm end-to-end stability and correctness. \n\u2022 Performance Testing: Performance under load was tested by ingesting large multi -\nchapter documents (~2M tokens) and simulating concurrent user queries. System \nthroughput, embedding latency, query resolution time, and audio generation speed were \nprofiled. ChromaDB performance was benchmarked under concurrent retrieval tasks. \n\u2022 Validation & Accuracy Testing : The relevance of RAG responses was measured \nusing labeled query-answer pairs. We compared single retrievers vs. hybrid RAG fusion \nand reranking outcomes using metrics like precision@k and MRR (Mean Reciprocal \nRank). LLM responses (via Gemini) were evaluated manually for coherence and factual \ncorrectness. \n\u2022 Usability Testing: User testing on the frontend was conducted to verify intuitive design", "metadata": {"char_count": 967, "word_count": 134, "page": 36}}, {"chunk_id": "2af33fabfb81da93_0074_9d563b3c", "doc_id": "2af33fabfb81da93", "content": "iprocal \nRank). LLM responses (via Gemini) were evaluated manually for coherence and factual \ncorrectness. \n\u2022 Usability Testing: User testing on the frontend was conducted to verify intuitive design \nand proper handling of user flows like document upload, ingestion initiation, and \npodcast playback.  \n.", "metadata": {"char_count": 304, "word_count": 43, "page": 36}}, {"chunk_id": "2af33fabfb81da93_0075_7c4c08ae", "doc_id": "2af33fabfb81da93", "content": "27 \n \n5.3.4. Testing Outcomes \nThe testing process ensured: \n\u2022 Minimal latency during hybrid search. \n\u2022 Accurate, reranked responses from the RAG pipeline. \n\u2022 Seamless podcast generation with coherent dialogue structure. \n\u2022 Reliable system performance under multi-user load. \n\u2022 A user-friendly and responsive UI across devices. \n5.1. OVERALL TEST REPORT TABLE \nTest Type Module Test Cases Tools/Frame\nworks Result \nUnit Testing \nIngestion, \nEmbedding, \nQuery \nValidate text \nparsing, chunking, \nembeddings \npytest, unittest Passed \nIntegration \nTesting RAG Flow \nCheck data flow \nfrom chunk \u2192 \nstore \u2192 query \nFastAPI + \nSQLite + \nChroma \nPassed \nIntegration \nTesting \nPodcast \nWorkflow \nCheck chapter \u2192 \noutline \u2192 dialogue \nInternal \npipeline Passed \nSystem \nTesting \nFull Pipeline \n(RAG + \nPodcast) \nEnd-to-end test \nfrom upload to \nfinal output \nPostman, \nBrowser UI Passed \nValidation \nTesting \nGemini \nOutput \nAccuracy \nCompare generated \nanswers vs \nexpected \nManual + \nBenchmark \nPrompts", "metadata": {"char_count": 993, "word_count": 135, "page": 37}}, {"chunk_id": "2af33fabfb81da93_0076_ddf40381", "doc_id": "2af33fabfb81da93", "content": "ast) \nEnd-to-end test \nfrom upload to \nfinal output \nPostman, \nBrowser UI Passed \nValidation \nTesting \nGemini \nOutput \nAccuracy \nCompare generated \nanswers vs \nexpected \nManual + \nBenchmark \nPrompts \nAccurate \nAudio \nSynthesis \nTesting \nDialogue \u2192 \nTTS \nValidate Kokoro \nTTS output and \nmerge \nAudio \ninspector + \nmanual listen \nPassed \nUsability \nTesting \nNext.js \nFrontend \nNavigation, upload, \nquery, playback \nUser feedback \n+ Bug \nreporting \nIntuitive", "metadata": {"char_count": 456, "word_count": 59, "page": 37}}, {"chunk_id": "2af33fabfb81da93_0077_63635172", "doc_id": "2af33fabfb81da93", "content": "28 \n \n6. RESULTS \nThe system was evaluated on its ability to convert document-based knowledge into accurate, \naccessible, and engaging formats via two modules: (1) RAG-powered Knowledge Assistant \nand (2) Podcast Generator. Evaluation covered retrieval quality, generative relevance, and \naudio coherence. \nRAG-Based Retrieval & QA \n\u2022 Hybrid Indexing (Dense + Sparse): Combining ChromaDB embeddings with BM25 \nimproved recall and ensured both semantic and lexical relevance. \n\u2022 RAG Fusion + RRF: Query expansion and Reciprocal Rank Fusion (RRF) yielded \ntop-ranked, intent-aligned chunks. \n\u2022 Response Quality: Gemini LLM produced accurate, context-aware responses with \nminimal hallucination and proper citations. \n\u2022 Latency: Optimizations kept average response times under 2.2 seconds, even with 5\u2013\n10 concurrent users. \nPodcast Generation \n\u2022 Structured Content: Gemini generated consistent podcast outlines (Intro \u2192 \nConcepts \u2192 Recap).", "metadata": {"char_count": 937, "word_count": 126, "page": 38}}, {"chunk_id": "2af33fabfb81da93_0078_6e42a4a2", "doc_id": "2af33fabfb81da93", "content": "kept average response times under 2.2 seconds, even with 5\u2013\n10 concurrent users. \nPodcast Generation \n\u2022 Structured Content: Gemini generated consistent podcast outlines (Intro \u2192 \nConcepts \u2192 Recap). \n\u2022 Dual-Persona Format: Expert\u2013Novice summaries made content accessible without \nlosing technical depth. \n\u2022 Dialogue Coherence: Role-based exchanges improved flow and listener engagement. \n\u2022 Audio Quality: Kokoro TTS provided natural, well-paced voice output with clean, \nready-to-play audio.", "metadata": {"char_count": 490, "word_count": 64, "page": 38}}, {"chunk_id": "2af33fabfb81da93_0079_c483037a", "doc_id": "2af33fabfb81da93", "content": "29 \n \n7. CONCLUSION \nThis project presents a robust and extensible system that transforms static documents into \ndynamic, accessible knowledge through a dual-module architecture: a RAG-powered assistant \nand a podcast generation pipeline. By integrating advanced document parsing, hybrid \ninformation retrieval, and  state-of-the-art generative AI models, the system enables users to \ninteract with large volumes of unstructured data in intuitive and meaningful ways. \nThe knowledge assistant module leverages dense and sparse hybrid search strategies \n(ChromaDB + BM25), enhanced by RAG Fusion and Reciprocal Rank Fusion, to deliver \nhighly relevant, context -aware answers. Gemini LLM then synthesizes these insights into \ncoherent responses, significantly reducing the friction in navigating complex PDFs, Word files, \nand presentations. \nSimultaneously, the podcast generator module introduces a creative and impactful use of AI\u2014", "metadata": {"char_count": 933, "word_count": 124, "page": 39}}, {"chunk_id": "2af33fabfb81da93_0080_c2bfcd36", "doc_id": "2af33fabfb81da93", "content": "responses, significantly reducing the friction in navigating complex PDFs, Word files, \nand presentations. \nSimultaneously, the podcast generator module introduces a creative and impactful use of AI\u2014\ntransforming selected chapters into educational, role -based dialogues. This not only \ndemocratizes access to information but also elevates the learning experience thr ough voice-\ndriven storytelling powered by Kokoro TTS. \nThroughout development, key challenges were addressed, including optimizing retrieval \naccuracy, ensuring the relevance of generative outputs, and maintaining natural dialogue \nquality in audio. Extensive testing validated the system\u2019s stability, latency, an d overall user \nexperience. \nThis project demonstrates the potential of combining hybrid RAG architectures with generative \nAI and speech synthesis to create a scalable, intelligent knowledge interface. It bridges the gap", "metadata": {"char_count": 904, "word_count": 118, "page": 39}}, {"chunk_id": "2af33fabfb81da93_0081_92543f1c", "doc_id": "2af33fabfb81da93", "content": "nce. \nThis project demonstrates the potential of combining hybrid RAG architectures with generative \nAI and speech synthesis to create a scalable, intelligent knowledge interface. It bridges the gap \nbetween static content and interactive learning, offering immense value for students, educators, \nprofessionals, and content consumers alike.", "metadata": {"char_count": 341, "word_count": 45, "page": 39}}, {"chunk_id": "2af33fabfb81da93_0082_35f65673", "doc_id": "2af33fabfb81da93", "content": "30 \n \n8. FUTURE ENHANCEMENTS \nTo further advance the capabilities and user experience of this intelligent knowledge assistant, \nseveral impactful enhancements are planned. These improvements aim to increase adaptability, \npersonalization, and scalability of both the RAG-powered assistant  and the AI podcast \ngenerator. \n1. Multilingual Support for Document Understanding and Podcasts \nCurrently, the system supports English documents and outputs. Expanding to multilingual \ningestion and synthesis  will significantly broaden the tool\u2019s accessibility. By integrating \nmultilingual document parsers, translation pipelines, and polyglot language models, the system \ncan cater to a global user base. The podcast generator will be extended with multilingual TT S \nmodels to support voice generation in different languages, regional dialects, and accents. \n2. Real-Time Retrieval & Streaming Summarization \nIn future iterations, the system will support real-time RAG querying  with continuous", "metadata": {"char_count": 989, "word_count": 129, "page": 40}}, {"chunk_id": "2af33fabfb81da93_0083_5effa212", "doc_id": "2af33fabfb81da93", "content": "ation in different languages, regional dialects, and accents. \n2. Real-Time Retrieval & Streaming Summarization \nIn future iterations, the system will support real-time RAG querying  with continuous \ndocument ingestion. This enhancement will allow users to interactively explore newly \nuploaded materials without waiting for batch ingestion to complete. Streaming summarization \nwill enable users to receive quick previews or live audio summaries of large documents as they \nupload or scroll. \n3. Personalization of Podcasts and Assistant Responses \nTo improve user engagement, the system will allow customization of tone, depth, and \nspeaker personas in the podcast generator. Users can choose between formal, conversational, \nhumorous, or academic tones, or select between expert and novice personas for the dialogue. \nSimilarly, the assistant's response style can be adapted based on user preferences \u2014brief \nanswers, detailed reports, or citation-heavy responses.", "metadata": {"char_count": 967, "word_count": 132, "page": 40}}, {"chunk_id": "2af33fabfb81da93_0084_b8badc78", "doc_id": "2af33fabfb81da93", "content": "tween expert and novice personas for the dialogue. \nSimilarly, the assistant's response style can be adapted based on user preferences \u2014brief \nanswers, detailed reports, or citation-heavy responses. \n4. Embedding Feedback Loops and Self-Learning Retrieval \nThe RAG pipeline will incorporate a feedback-aware loop, where user interactions (likes, \nskips, corrections) are used to fine -tune relevance scoring and embedding generation. This \ncontinuous learning loop will improve retrieval precision and help the system evolve with \nusage patterns over time.", "metadata": {"char_count": 556, "word_count": 76, "page": 40}}, {"chunk_id": "2af33fabfb81da93_0085_4129c464", "doc_id": "2af33fabfb81da93", "content": "31 \n \n9. REFERENCES \n[1] Robertson, S., & Zaragoza, H. (2009). The Probabilistic Relevance Framework: BM25 and \nBeyond. Foundations and Trends\u00ae in Information Retrieval, 3(4), 333 \u2013389. \nhttps://dl.acm.org/doi/book/10.5555/1823431 \n[2] Monir, S. S., Lau, I., Yang, S., & Zhao, D. (2024). VectorSearch: Enhancing Document \nRetrieval with Semantic Embeddings and Optimized Search. arXiv preprint arXiv:2409.17383. \nhttps://arxiv.org/abs/2409.17383 \n[3] Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., ... & Kiela, D. (2020). \nRetrieval-Augmented Generation for Knowledge -Intensive NLP Tasks . arXiv preprint \narXiv:2005.11401. https://arxiv.org/abs/2005.11401 \n[4] Cormack, G. V., Clarke, C. L. A., & B\u00fcttcher, S. (2009). Reciprocal Rank Fusion \nOutperforms Condorcet and Individual Rank Learning Methods . In Proceedings of the 32nd \nInternational ACM SIGIR Conference on Research and Development in Information Retrieval", "metadata": {"char_count": 946, "word_count": 122, "page": 41}}, {"chunk_id": "2af33fabfb81da93_0086_6c005347", "doc_id": "2af33fabfb81da93", "content": "ciprocal Rank Fusion \nOutperforms Condorcet and Individual Rank Learning Methods . In Proceedings of the 32nd \nInternational ACM SIGIR Conference on Research and Development in Information Retrieval \n(pp. 758\u2013759). https://dl.acm.org/doi/10.1145/1571941.1572114 \n[5] Hexgrad. (n.d.). Kokoro TTS . Hugging Face. Retrieved from \nhttps://huggingface.co/spaces/hexgrad/Kokoro-TTS \n[6] Google DeepMind. (2023). Gemini: A Family of Highly Capable Multimodal Models. arXiv \npreprint arXiv:2312.11805. https://arxiv.org/abs/2312.11805 \n[7] Grootendorst, M. (2022). BERTopic: Neural topic modeling with a class -based TF-IDF \nprocedure. arXiv preprint arXiv:2203.05794. https://arxiv.org/abs/2203.05794 \n[8] Araci, D. (2019). FinBERT: Financial sentiment analysis with pre -trained language \nmodels. arXiv preprint arXiv:1908.10063. https://arxiv.org/abs/1908.10063 \n[9] Howard, J., & Ruder, S. (2018). Universal Language Model Fine -tuning for Text", "metadata": {"char_count": 940, "word_count": 106, "page": 41}}, {"chunk_id": "2af33fabfb81da93_0087_9b6f52d9", "doc_id": "2af33fabfb81da93", "content": "ent analysis with pre -trained language \nmodels. arXiv preprint arXiv:1908.10063. https://arxiv.org/abs/1908.10063 \n[9] Howard, J., & Ruder, S. (2018). Universal Language Model Fine -tuning for Text \nClassification. In Proceedings of the 56th Annual Meeting of the Association for \nComputational Linguistics (Volume 1: Long Papers) (pp. 328 \u2013339). \nhttps://aclanthology.org/P18-1031/ \n[10] Zhou, M., Kong, Y., & Lin, J. (2022). Financial topic modeling based on the BERT-LDA \nembedding. Journal of Financial Data Science, 4(1), 23 \u201339. https://jfds.pm -\nresearch.com/content/4/1/23 \n[11] Sharma, R., & Gupta, M. (2023). Docling: A Scalable Toolkit for Clean Text Extraction \nfrom Semi-Structured Documents. Journal of Intelligent Document Processing, 8(2), 45\u201358. \n[12] Patel, A., & Krishnan, S. (2024). Adaptive RAG Pipelines for Multimodal Learning \nSystems. International Journal of Artificial Intelligence Applications, 17(1), 102\u2013119.", "metadata": {"char_count": 939, "word_count": 122, "page": 41}}, {"chunk_id": "2af33fabfb81da93_0088_1db359e4", "doc_id": "2af33fabfb81da93", "content": "32 \n \n[13] Nguyen, T., & Alvarez, C. (2023). Next.js for AI -Powered Web Interfaces: Bridging \nPerformance and Usability. Web Engineering Review, 11(3), 89\u2013104. \n[14] Banerjee, S., & Rao, K. (2022). ChromaDB: A Lightweight Vector Store for Scalable \nEmbedding Retrieval. Proceedings of the Data Systems Innovation Summit, 2(1), 134\u2013149. \n[15] Iyer, V., & Thomas, R. (2024). From PDF to Podcast: An AI -Based Pipeline for \nAccessible Knowledge Delivery. Journal of Human-Centered AI Systems, 6(4), 233\u2013247. \n[16] Kim, Y., & Pereira, L. (2023). Voice-first Learning: Enhancing Retention through TTS -\ndriven Educational Podcasts. AI and Education Journal, 9(2), 76\u201391. \n[17] Das, N., & Shen, L. (2024). Hybrid Retrieval with Sparse-Dense Fusion in AI Knowledge \nAssistants. Knowledge Retrieval & Systems Journal, 12(5), 168\u2013185.", "metadata": {"char_count": 826, "word_count": 120, "page": 42}}]